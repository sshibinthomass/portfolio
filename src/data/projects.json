{
    "en": [
        {
            "id": 1,
            "title": "Development of LLM-Agent for eCADSTAR",
            "description": "AI-driven circuit design agent with vision-language models and autonomous simulation",
            "images": [
                "/portfolio/images/project_1.jpg"
            ],
            "technologies": [
                "Qwen2.5-VL",
                "Granite VLM",
                "LoRA",
                "ChromaDB",
                "LangChain",
                "Ollama",
                "Python",
                "SPICE"
            ],
            "youtubeLink": "",
            "github": "",
            "appLink": "",
            "purpose": "TU Dortmund Group Project",
            "duration": "8 months",
            "date": "September 2024-April 2025",
            "detailedDescription": "<h3>Introduction & Motivation</h3><p>Built a next-generation AI chatbot to support engineers with PCB design tasks, combining natural language processing, vision models, simulation APIs, and information retrieval. Traditional LLMs were limited to static text and prone to hallucination; this system is multimodal and tool-augmented, capable of handling complex tasks in signal integrity (SI), power integrity (PI), and electromagnetic compatibility (EMC).</p><h3>Vision-Language Models (Image Descriptor)</h3><p>Evaluated and fine-tuned multiple models for interpreting circuit schematics and simulation graphs:</p><ul><li><strong>Qwen2.5-VL:</strong> Best performer overall with high accuracy and detailed recognition of components, SI/FD/TDR graphs</li><li><strong>Granite-Vision-3.2:</strong> Strong contender for both circuit and waveform images with excellent shape-based recognition</li><li><strong>SmolVLM & 500M:</strong> Lightweight but shallow outputs and frequent hallucinations</li><li><strong>LLaVA-1.6-Mistral:</strong> Incorrect topologies and severe hallucinations. Not suitable for engineering images</li></ul><p>Each model was fine-tuned using LoRA adapters and 4-bit quantization to run within 48GB GPU limits.</p><h3>Dataset Preparation</h3><ul><li>Captured <strong>4500+ annotated samples</strong> from eCADSTAR software (circuit schematics + SI/FD/TDR waveform images)</li><li>Augmented dataset with component-only images to improve object recognition</li><li>Used <strong>Albumentations</strong> for image augmentation (rotation, flips, emboss, brightness)</li><li>Used <strong>Gemini and Qwen2.5</strong> for text augmentation (paraphrasing, synonym replacement)</li><li>Data formatted in JSON to support image-query-answer format for VLM fine-tuning</li></ul><h3>Fine-Tuning Process</h3><ul><li>Implemented Low-Rank Adaptation (LoRA) and quantization to fit models within VRAM constraints</li><li>Used a template with system messages prompting models to identify topology, components, waveform behavior</li><li>Used 20% of the dataset for evaluation with loss/accuracy tracked using TensorBoard</li><li>Final results: Qwen2.5-VL and Granite-Vision-3.2 achieved best loss (<3) and mean token accuracy (>85%), while LLaVA had very high loss and poor generalization</li></ul><h3>RAG (Retrieval-Augmented Generation)</h3><ul><li><strong>LangChain:</strong> Used as orchestration backbone connecting tools and LLMs</li><li><strong>ChromaDB:</strong> Vector store for all file-based and web-based text chunks</li><li><strong>Embedding Models:</strong> Nomic-Embed-Text and GPT4AllEmbeddings</li><li><strong>File Types:</strong> Supported PDF, DOCX, XLSX, CSV, PPTX</li><li><strong>Web Retrieval:</strong> Used DuckDuckGo, Bing, Google, and URL scraping for real-time knowledge access</li><li><strong>Hybrid Retrieval:</strong> Combined web + file content to build enriched prompts for LLMs</li><li><strong>Performance:</strong> Sub-second retrieval even at 100K vectors, integrated seamlessly into the chatbot</li></ul><h3>AI Agent & Orchestrator System</h3><ul><li>Used LangChain's React Agent design with structured reasoning + tool calling steps</li><li>Agent Inputs handled image-only, query-only, and image + query combinations</li><li><strong>Memory:</strong> Short-term via LangChain Buffer, Long-term via persistent vector DB history</li><li><strong>Tools Integrated:</strong><ul><li>PCB Expert: LLaMA 8B model for PCB queries</li><li>Simulation API: Triggered eCADSTAR via PyAutoGUI for SI, FD, TDR runs</li><li>RAG Engine: Provided external document/web context when needed</li></ul></li></ul><h3>Final Outcomes</h3><ul><li>Developed a fully integrated multi-modal chatbot for PCB engineers</li><li>Best performing model: <strong>Granite-Vision-3.2</strong> for circuit + waveform images</li><li>Robust RAG + Simulation toolchain enabled fact-based and analytical reasoning</li><li>System now supports simulation requests, layout validation, and real-time Q&A via web search or file analysis</li></ul>"
        },
        {
            "id": 2,
            "title": "SafeMate (Employee Management and Surveillance Bot)",
            "description": "Advanced facial recognition and surveillance system with IoT integration",
            "images": [
                "/portfolio/images/project_2_1.jpg"
            ],
            "technologies": [
                "OpenCV",
                "Facial Recognition",
                "Raspberry Pi",
                "Django",
                "Wolfram Alpha",
                "Wikipedia API",
                "Python"
            ],
            "youtubeLink": "https://youtu.be/QqrCg2vfLW0",
            "github": "",
            "appLink": "",
            "purpose": "Bachelor's Final Project",
            "duration": "1 Year",
            "date": "March 2022",
            "detailedDescription": "<h3>Funding & Recognition</h3><p>This project was officially funded under the Student Projects Scheme (SPS) by the Tamil Nadu State Council for Science and Technology (TANSCST), an initiative under the Government of Tamil Nadu, during the academic year 2021–2022.</p><p>The TANSCST Student Projects Scheme is a competitive funding program that supports innovative and socially impactful final-year student projects in science, engineering, and technology. Our project, titled 'Design and Fabrication of DRO-CO Bot', was selected after a rigorous evaluation of its technical viability, innovation, societal relevance, and feasibility.</p><p>The financial grant provided by TANSCST was utilized to design, prototype, and fabricate an AI-powered humanoid robot equipped with functionalities such as contactless temperature sensing, face mask detection, real-time face recognition for attendance, voice-based AI interaction, and autonomous SOP compliance enforcement.</p><p>This recognition not only validated the technical and social importance of the DRO-CO Bot but also helped us transform our concept into a fully working prototype tested in an industrial environment. Being funded by TANSCST significantly enhanced the visibility and credibility of our project at the state level.</p><p>The successful execution of this government-supported initiative highlights my ability to independently develop, manage, and deliver impactful engineering solutions under real-world constraints.</p><h3>Problem Statement</h3><p>In response to the COVID-19 pandemic, there was an urgent requirement for contactless systems to enforce Standard Operating Procedures (SOPs) like temperature checks, mask compliance, and attendance in public environments. Manual enforcement was risky and inefficient. The challenge was to build a cost-effective, smart, and autonomous robot to assist in enforcing safety rules and interact with users without human intervention.</p><h3>Project Summary</h3><p>I conceptualized, designed, programmed, and fabricated a humanoid robot named DRO-CO Bot, an AI-powered, IoT-integrated robotic assistant. It can detect face masks, measure temperature without contact, recognize faces for automatic attendance, and interact with users using speech recognition and AI-driven responses. This system reduces human labor, enforces COVID-19 SOPs, and serves as a personal assistant in offices, public spaces, and educational institutions.</p><h3>Project Objectives</h3><ul><li>Design a humanoid robot integrated with AI and IoT functionalities</li><li>Implement real-time mask detection and face recognition</li><li>Enable contactless temperature sensing and automatic attendance marking</li><li>Integrate speech-based AI assistant functionalities using APIs</li><li>Develop a cost-effective, scalable, and safe system for public use</li></ul><h3>Features and Capabilities</h3><ul><li>Face mask detection using TensorFlow and MobileNetV2</li><li>Real-time face recognition using OpenCV and face_recognition library</li><li>Contactless temperature sensing using MLX90614 IR sensor</li><li>Speech recognition and voice response using Python, pyttsx3, and Google Speech API</li><li>AI-powered assistant with Wikipedia, WolframAlpha, and OpenWeather integrations</li><li>WhatsApp automation via Selenium for live reporting and alerts</li><li>Servo motor-based robotic arm with 3 DOF controlled by Arduino (ATmega2560)</li><li>Ultrasonic-based object detection and crowd monitoring</li><li>Real-time attendance and temperature data logging using CSV and dashboard</li></ul><h3>Technical Implementation</h3><ul><li>Developed both forward and inverse kinematics for the robotic arm using D-H parameters</li><li>Calculated torque, stress, strain, buckling load for robot joints using PLA structural material</li><li>Designed and 3D printed all mechanical components using Fusion 360 and PLA filament</li><li>Established I2C communication between Raspberry Pi and Arduino for sensor control</li><li>Used servo motors (MG996R, RDS3115MG) for joint control with PID-based position feedback</li><li>Set up Raspberry Pi as the master node for camera, display, temperature, and voice processing</li><li>Built Python modules for face encoding, recognition, and voice-to-text interaction</li><li>Integrated audio-visual output via 5” HDMI LCD display and Bluetooth speaker</li></ul><h3>Engineering Design & Analysis</h3><ul><li>Degree of Freedom (DOF): 3 for arm, 1 for head</li><li>Forward Kinematics Equations for X and Y-plane positioning of the arm</li><li>Inverse Kinematics based on end-effector coordinates using trigonometric relations</li><li>Torque calculations for each joint using dynamic and static loads</li><li>Stress-Strain Analysis using Young’s Modulus and Safety Factor for PLA</li><li>Euler Buckling check for column rigidity to ensure structural safety</li><li>Bending Moment and Shear Force analysis for robotic arm and shoulder stability</li></ul><h3>Outcome</h3><ul><li>Successfully fabricated and tested a fully functional autonomous COVID-19 SOP enforcement robot</li><li>Recognized faces and recorded attendance with >95% accuracy</li><li>Detected mask violations in real-time and notified users via WhatsApp</li><li>Voice assistant answered queries, fetched weather, and gave system responses</li><li>Achieved smooth and accurate robotic arm motion with no structural failure</li><li>Tested successfully in a live industrial environment</li></ul><h3>Project Budget</h3><p>Total Cost: ₹28,601 INR (Optimized through open-source tools and affordable components)</p><h3>Future Enhancements</h3><ul><li>Extend surveillance to entire room using multiple camera feeds</li><li>BLE-based contact tracing and interaction logging</li><li>Solar power integration for sustainable off-grid operation</li><li>Multilingual and emotion-aware speech assistant</li><li>Human-following mode using real-time person tracking and mobile base</li></ul><h3>Technologies Used</h3><ul><li><strong>Hardware:</strong> Raspberry Pi 4, ATMEGA 2560, MLX90614, Ultrasonic Sensors, Servo Motors</li><li><strong>Programming:</strong> Python, C++, Arduino IDE</li><li><strong>Libraries:</strong> OpenCV, TensorFlow, face_recognition, Selenium, pyttsx3, Wikipedia, WolframAlpha API</li><li><strong>CAD & Fabrication:</strong> Fusion 360, PLA 3D Printing</li><li><strong>Protocols:</strong> I2C, GPIO, HDMI, USB, Bluetooth</li><li><strong>Others:</strong> CSV data logging, WhatsApp Web automation, TFT display interface</li></ul>"
        },
        {
            "id": 3,
            "title": "YogaVision (AI-powered IOT-based VR box with Application)",
            "description": "VR yoga application with AI-driven biometric monitoring and adaptive environments",
            "images": [
                "/portfolio/images/achievements/YogaVision1.jpg",
                "/portfolio/images/achievements/YogaVision3.jpg",
                "/portfolio/images/achievements/YogaVision4.jpg"
            ],
            "technologies": [
                "VR",
                "Eye-tracking",
                "IoT",
                "Particle Photon",
                "Biometric Sensors",
                "AI",
                "Cloud Computing"
            ],
            "youtubeLink": "https://youtu.be/F9xCMl1qXHo",
            "github": "",
            "appLink": "",
            "purpose": "SIH (Hardware edition)",
            "duration": "5 months",
            "date": "",
            "detailedDescription": "<h3>Introduction</h3><p>Yoga is an ancient practice aimed at harmonizing body, mind, and soul, offering benefits like flexibility, strength, and mental health. However, challenges like finding time, space, or instructors, and maintaining focus can hinder practice.</p><h3>Solution: YogaVision VR</h3><p>To address these challenges, we developed a smart, interactive VR device designed to enhance the yoga experience. Unlike standard headsets, it adapts to your needs and actively guides your journey.</p><h3>Key Features</h3><ul><li><strong>Lightweight & Durable Headset:</strong> Flexible material with cotton cushioning for pressure-free comfort.</li><li><strong>Next-Gen Lenses:</strong> Wide field of view with reduced glare for crystal-clear HD optics.</li><li><strong>LRPF Filters & IPD Adjuster:</strong> Blue light protection, contrast enhancement, and customizable lens distance.</li><li><strong>Inbuilt Spatial Audio:</strong> Integrated speakers providing immersive 3D soundscapes.</li><li><strong>Pulse Rate Monitoring:</strong> Real-time pulse and temperature sensors to track wellness and adjust intensity.</li><li><strong>Controller-less VR:</strong> Hands-free interaction using head movements, gaze, voice, and gestures.</li><li><strong>Interactive Virtual Tutor:</strong> Voice-responsive AI tutor providing personalized feedback, tips, and encouragement.</li><li><strong>Gaze Interaction:</strong> Navigate menus and change environments (e.g., Mountain to Beach) simply by looking at icons.</li><li><strong>Gyroscopic Walk:</strong> Explore 360-degree environments with a simple 30-degree head tilt—no controllers needed.</li><li><strong>3D Virtual Buttons:</strong> Intuitive controls for playback and settings within the VR space.</li><li><strong>Simple UI:</strong> User-friendly interface accessible for all ages.</li></ul><h3>Use Cases</h3><ul><li><strong>Employee Wellness:</strong> Reduces stress and burnout by offering an accessible escape and relaxation tool.</li><li><strong>Student Education:</strong> Interactive learning to develop concentration, memory, and physical health.</li><li><strong>Lifelong Learning:</strong> Personalized, adaptive yoga sessions for continuous self-improvement and well-being.</li></ul><h3>Conclusion</h3><p>We believe YogaVision can revolutionize yoga practice through innovation, inspiring learners of all backgrounds to achieve their physical and mental goals.</p>"
        }
    ],
    "de": [
        {
            "id": 1,
            "title": "Entwicklung eines LLM-Agenten für eCADSTAR",
            "description": "KI-gesteuerter Schaltungsdesign-Agent mit Vision-Language-Modellen und autonomer Simulation",
            "detailedDescription": "<h3>Einführung & Motivation</h3><p>Entwicklung eines KI-Chatbots der nächsten Generation zur Unterstützung von Ingenieuren bei PCB-Design-Aufgaben, der natürliche Sprachverarbeitung, Vision-Modelle, Simulations-APIs und Informationsabruf kombiniert. Traditionelle LLMs waren auf statischen Text beschränkt und anfällig für Halluzinationen; dieses System ist multimodal und tool-augmentiert, fähig zur Bewältigung komplexer Aufgaben in Signal Integrity (SI), Power Integrity (PI) und elektromagnetischer Verträglichkeit (EMV).</p><h3>Vision-Language-Modelle (Bildbeschreiber)</h3><p>Evaluierung und Feinabstimmung mehrerer Modelle zur Interpretation von Schaltplänen und Simulationsgraphen:</p><ul><li><strong>Qwen2.5-VL:</strong> Bester Gesamtperformer mit hoher Genauigkeit und detaillierter Erkennung von Komponenten, SI/FD/TDR-Graphen</li><li><strong>Granite-Vision-3.2:</strong> Starker Kandidat für Schaltungs- und Wellenformbilder mit exzellenter formbasierter Erkennung</li><li><strong>SmolVLM & 500M:</strong> Leichtgewichtig, aber oberflächliche Ausgaben und häufige Halluzinationen</li><li><strong>LLaVA-1.6-Mistral:</strong> Inkorrekte Topologien und schwere Halluzinationen. Nicht geeignet für technische Bilder</li></ul><p>Jedes Modell wurde mittels LoRA-Adaptern und 4-Bit-Quantisierung feinabgestimmt, um innerhalb von 48GB GPU-Limits zu laufen.</p><h3>Datensatz-Vorbereitung</h3><ul><li>Erfassung von <strong>4500+ annotierten Samples</strong> aus der eCADSTAR-Software (Schaltpläne + SI/FD/TDR-Wellenformbilder)</li><li>Erweiterung des Datensatzes mit reinen Komponentenbildern zur Verbesserung der Objekterkennung</li><li>Verwendung von <strong>Albumentations</strong> für Bildaugmentierung (Rotation, Spiegelung, Prägung, Helligkeit)</li><li>Verwendung von <strong>Gemini und Qwen2.5</strong> für Textaugmentierung (Paraphrasierung, Synonymersetzung)</li><li>Daten im JSON-Format formatiert zur Unterstützung des Bild-Abfrage-Antwort-Formats für VLM-Feinabstimmung</li></ul><h3>Feinabstimmungsprozess</h3><ul><li>Implementierung von Low-Rank Adaptation (LoRA) und Quantisierung zur Anpassung der Modelle an VRAM-Beschränkungen</li><li>Verwendung eines Templates mit Systemnachrichten zur Aufforderung an Modelle, Topologie, Komponenten und Wellenformverhalten zu identifizieren</li><li>Verwendung von 20% des Datensatzes für die Evaluierung mit Loss/Accuracy-Tracking mittels TensorBoard</li><li>Endergebnisse: Qwen2.5-VL und Granite-Vision-3.2 erzielten besten Loss (<3) und mittlere Token-Genauigkeit (>85%), während LLaVA sehr hohen Loss und schlechte Generalisierung aufwies</li></ul><h3>RAG (Retrieval-Augmented Generation)</h3><ul><li><strong>LangChain:</strong> Verwendung als Orchestrierungs-Backbone zur Verbindung von Tools und LLMs</li><li><strong>ChromaDB:</strong> Vektorspeicher für alle datei- und webbasierten Textchunks</li><li><strong>Embedding-Modelle:</strong> Nomic-Embed-Text und GPT4AllEmbeddings</li><li><strong>Dateitypen:</strong> Unterstützte PDF, DOCX, XLSX, CSV, PPTX</li><li><strong>Web-Abruf:</strong> Verwendung von DuckDuckGo, Bing, Google und URL-Scraping für Echtzeit-Wissenszugriff</li><li><strong>Hybrid-Retrieval:</strong> Kombination von Web- + Dateiinhalten zum Aufbau angereicherter Prompts für LLMs</li><li><strong>Performance:</strong> Sub-Sekunden-Abruf selbst bei 100K Vektoren, nahtlos in den Chatbot integriert</li></ul><h3>KI-Agent & Orchestrator-System</h3><ul><li>Verwendung von LangChains React-Agent-Design mit strukturiertem Reasoning + Tool-Calling-Schritten</li><li>Agent-Inputs behandelten Nur-Bild, Nur-Abfrage und Bild + Abfrage-Kombinationen</li><li><strong>Speicher:</strong> Kurzzeit via LangChain Buffer, Langzeit via persistente Vektor-DB-Historie</li><li><strong>Integrierte Tools:</strong><ul><li>PCB Expert: LLaMA 8B-Modell für PCB-Abfragen</li><li>Simulations-API: Auslösung von eCADSTAR via PyAutoGUI für SI, FD, TDR-Läufe</li><li>RAG-Engine: Bereitstellung von externem Dokument-/Web-Kontext bei Bedarf</li></ul></li></ul><h3>Endergebnisse</h3><ul><li>Entwicklung eines vollständig integrierten multimodalen Chatbots für PCB-Ingenieure</li><li>Bestes Modell: <strong>Granite-Vision-3.2</strong> für Schaltungs- + Wellenformbilder</li><li>Robuste RAG + Simulations-Toolchain ermöglichte faktenbasiertes und analytisches Reasoning</li><li>System unterstützt jetzt Simulationsanfragen, Layout-Validierung und Echtzeit-Q&A via Websuche oder Dateianalyse</li></ul>",
            "purpose": "TU Dortmund Gruppenprojekt",
            "duration": "8 Monate",
            "date": "September 2024 - April 2025"
        },
        {
            "id": 2,
            "title": "SafeMate (Employee Management und Surveillance Bot)",
            "description": "Fortschrittliches Gesichtserkennungs- und Überwachungssystem mit IoT-Integration",
            "detailedDescription": "<h3>Finanzierung & Anerkennung</h3><p>Dieses Projekt wurde im akademischen Jahr 2021–2022 offiziell im Rahmen des Student Projects Scheme (SPS) vom Tamil Nadu State Council for Science and Technology (TANSCST), einer Initiative der Regierung von Tamil Nadu, gefördert.</p><p>Das TANSCST Student Projects Scheme ist ein wettbewerbsorientiertes Förderprogramm, das innovative und gesellschaftlich relevante Abschlussprojekte von Studenten in Wissenschaft, Technik und Technologie unterstützt. Unser Projekt mit dem Titel 'Design and Fabrication of DRO-CO Bot' wurde nach einer strengen Bewertung seiner technischen Machbarkeit, Innovation, gesellschaftlichen Relevanz und Umsetzbarkeit ausgewählt.</p><p>Der finanzielle Zuschuss des TANSCST wurde genutzt, um einen KI-gesteuerten humanoiden Roboter zu entwerfen, als Prototyp zu bauen und herzustellen. Er ist mit Funktionen wie kontaktloser Temperaturmessung, Gesichtsmaskenerkennung, Echtzeit-Gesichtserkennung für die Anwesenheit, sprachbasierter KI-Interaktion und autonomer Durchsetzung von SOP-Regeln ausgestattet.</p><p>Diese Anerkennung bestätigte nicht nur die technische und soziale Bedeutung des DRO-CO Bots, sondern half uns auch, unser Konzept in einen voll funktionsfähigen Prototypen umzuwandeln, der in einem industriellen Umfeld getestet wurde. Die Förderung durch den TANSCST hat die Sichtbarkeit und Glaubwürdigkeit unseres Projekts auf Landesebene deutlich erhöht.</p><p>Die erfolgreiche Durchführung dieser staatlich geförderten Initiative unterstreicht meine Fähigkeit, wirkungsvolle technische Lösungen unter realen Bedingungen unabhängig zu entwickeln, zu verwalten und bereitzustellen.</p><h3>Problemstellung</h3><p>Als Reaktion auf die COVID-19-Pandemie bestand ein dringender Bedarf an kontaktlosen Systemen zur Durchsetzung von Standardarbeitsanweisungen (SOPs) wie Temperaturkontrollen, Maskenpflicht und Anwesenheit in öffentlichen Umgebungen. Manuelle Durchsetzung war riskant und ineffizient. Die Herausforderung bestand darin, einen kostengünstigen, intelligenten und autonomen Roboter zu bauen, der bei der Durchsetzung von Sicherheitsregeln hilft und ohne menschliches Eingreifen mit Benutzern interagiert.</p><h3>Projektzusammenfassung</h3><p>Ich habe einen humanoiden Roboter namens DRO-CO Bot konzipiert, entworfen, programmiert und gefertigt – einen KI-gesteuerten, IoT-integrierten Roboterassistenten. Er kann Gesichtsmasken erkennen, Temperatur kontaktlos messen, Gesichter für die automatische Anwesenheit erkennen und über Spracherkennung und KI-gesteuerte Antworten mit Benutzern interagieren. Dieses System reduziert menschliche Arbeit, setzt COVID-19-SOPs durch und dient als persönlicher Assistent in Büros, öffentlichen Räumen und Bildungseinrichtungen.</p><h3>Projektziele</h3><ul><li>Entwurf eines humanoiden Roboters mit integrierten KI- und IoT-Funktionen</li><li>Implementierung von Echtzeit-Maskenerkennung und Gesichtserkennung</li><li>Ermöglichung kontaktloser Temperaturmessung und automatischer Anwesenheitserfassung</li><li>Integration sprachbasierter KI-Assistentenfunktionen über APIs</li><li>Entwicklung eines kostengünstigen, skalierbaren und sicheren Systems für den öffentlichen Gebrauch</li></ul><h3>Funktionen und Fähigkeiten</h3><ul><li>Gesichtsmaskenerkennung mittels TensorFlow und MobileNetV2</li><li>Echtzeit-Gesichtserkennung mittels OpenCV und face_recognition Bibliothek</li><li>Kontaktlose Temperaturmessung mittels MLX90614 IR-Sensor</li><li>Spracherkennung und Sprachantwort mit Python, pyttsx3 und Google Speech API</li><li>KI-gestützter Assistent mit Wikipedia, WolframAlpha und OpenWeather Integrationen</li><li>WhatsApp-Automatisierung über Selenium für Live-Berichte und Warnungen</li><li>Servomotor-basierter Roboterarm mit 3 Freiheitsgraden (DOF), gesteuert durch Arduino (ATmega2560)</li><li>Ultraschallbasierte Objekterkennung und Überwachung von Menschenmengen</li><li>Echtzeit-Protokollierung von Anwesenheits- und Temperaturdaten mittels CSV und Dashboard</li></ul><h3>Technische Umsetzung</h3><ul><li>Entwicklung von Vorwärts- und Inverskinematik für den Roboterarm unter Verwendung von D-H-Parametern</li><li>Berechnung von Drehmoment, Spannung, Dehnung und Knicklast für Robotergelenke unter Verwendung von PLA-Strukturmaterial</li><li>Konstruktion und 3D-Druck aller mechanischen Komponenten mit Fusion 360 und PLA-Filament</li><li>Einrichtung der I2C-Kommunikation zwischen Raspberry Pi und Arduino zur Sensorsteuerung</li><li>Verwendung von Servomotoren (MG996R, RDS3115MG) für die Gelenksteuerung mit PID-basiertem Positionsfeedback</li><li>Einrichtung des Raspberry Pi als Master-Node für Kamera-, Display-, Temperatur- und Sprachverarbeitung</li><li>Erstellung von Python-Modulen für Face Encoding, Erkennung und Voice-to-Text-Interaktion</li><li>Integration von audiovisueller Ausgabe über 5” HDMI LCD-Display und Bluetooth-Lautsprecher</li></ul><h3>Technische Konstruktion & Analyse</h3><ul><li>Freiheitsgrade (DOF): 3 für den Arm, 1 für den Kopf</li><li>Vorwärtskinematik-Gleichungen für die Positionierung des Arms in der X- und Y-Ebene</li><li>Inverskinematik basierend auf Endeffektor-Koordinaten unter Verwendung trigonometrischer Beziehungen</li><li>Drehmomentberechnungen für jedes Gelenk unter Verwendung dynamischer und statischer Lasten</li><li>Spannungs-Dehnungs-Analyse unter Verwendung des Elastizitätsmoduls und des Sicherheitsfaktors für PLA</li><li>Euler-Knickprüfung für die Säulensteifigkeit zur Gewährleistung der strukturellen Sicherheit</li><li>Biegemoment- und Querkraftanalyse für die Stabilität von Roboterarm und Schulter</li></ul><h3>Ergebnis</h3><ul><li>Erfolgreiche Herstellung und Prüfung eines voll funktionsfähigen autonomen Roboters zur Durchsetzung von COVID-19-SOPs</li><li>Erkannte Gesichter und erfasste Anwesenheit mit >95% Genauigkeit</li><li>Erkannte Maskenverstöße in Echtzeit und benachrichtigte Benutzer über WhatsApp</li><li>Sprachassistent beantwortete Anfragen, rief Wetterdaten ab und gab Systemantworten</li><li>Erzielte reibungslose und präzise Bewegung des Roboterarms ohne strukturelles Versagen</li><li>Erfolgreich in einer Live-Industrieumgebung getestet</li></ul><h3>Projektbudget</h3><p>Gesamtkosten: ₹28,601 INR (Optimiert durch Open-Source-Tools und erschwingliche Komponenten)</p><h3>Zukünftige Erweiterungen</h3><ul><li>Erweiterung der Überwachung auf den gesamten Raum durch mehrere Kamera-Feeds</li><li>BLE-basiertes Contact Tracing und Interaktionsprotokollierung</li><li>Solarstromintegration für nachhaltigen netzunabhängigen Betrieb</li><li>Mehrsprachiger und emotionsbewusster Sprachassistent</li><li>Menschenfolgemodus mittels Echtzeit-Personenverfolgung und mobiler Basis</li></ul><h3>Verwendete Technologien</h3><ul><li><strong>Hardware:</strong> Raspberry Pi 4, ATMEGA 2560, MLX90614, Ultraschallsensoren, Servomotoren</li><li><strong>Programmierung:</strong> Python, C++, Arduino IDE</li><li><strong>Bibliotheken:</strong> OpenCV, TensorFlow, face_recognition, Selenium, pyttsx3, Wikipedia, WolframAlpha API</li><li><strong>CAD & Fertigung:</strong> Fusion 360, PLA 3D-Druck</li><li><strong>Protokolle:</strong> I2C, GPIO, HDMI, USB, Bluetooth</li><li><strong>Andere:</strong> CSV-Datenprotokollierung, WhatsApp Web-Automatisierung, TFT-Display-Schnittstelle</li></ul>",
            "purpose": "Bachelor-Abschlussprojekt",
            "duration": "1 Jahr",
            "date": "März 2022"
        },
        {
            "id": 3,
            "title": "YogaVision (KI-gestützte IOT-basierte VR-Box mit Anwendung)",
            "description": "VR-Yoga-Anwendung mit KI-gesteuerter biometrischer Überwachung und adaptiven Umgebungen",
            "detailedDescription": "<h3>Einführung</h3><p>Yoga ist eine uralte Praxis, die darauf abzielt, Körper, Geist und Seele in Einklang zu bringen und Vorteile wie Flexibilität, Kraft und psychische Gesundheit bietet. Herausforderungen wie Zeitmangel, fehlender Raum oder fehlende Lehrer sowie Konzentrationsschwierigkeiten können die Praxis jedoch behindern.</p><h3>Lösung: YogaVision VR</h3><p>Um diesen Herausforderungen zu begegnen, haben wir ein intelligentes, interaktives VR-Gerät entwickelt, das speziell zur Verbesserung des Yoga-Erlebnisses konzipiert wurde. Anders als herkömmliche Headsets passt es sich Ihren Bedürfnissen an und führt Sie aktiv durch Ihre Reise.</p><h3>Hauptmerkmale</h3><ul><li><strong>Leichtes & langlebiges Headset:</strong> Flexibles Material mit Baumwollpolsterung für druckfreien Komfort.</li><li><strong>Linsen der nächsten Generation:</strong> Breites Sichtfeld mit deutlich reduzierter Blendung für kristallklare HD-Optik.</li><li><strong>LRPF-Filter & IPD-Einsteller:</strong> Blaulichtschutz, Kontrastverbesserung und anpassbarer Linsenabstand.</li><li><strong>Eingebautes Spatial Audio:</strong> Integrierte Lautsprecher für immersive 3D-Klanglandschaften.</li><li><strong>Herzfrequenzüberwachung:</strong> Echtzeit-Puls- und Temperatursensoren zur Überwachung des Wohlbefindens und Anpassung der Intensität.</li><li><strong>Controllerloses VR:</strong> Freihändige Interaktion durch Kopfbewegungen, Blicksteuerung, Stimme und Gesten.</li><li><strong>Interaktiver virtueller Tutor:</strong> Sprachgesteuerte KI, die personalisiertes Feedback, Tipps und Ermutigung bietet.</li><li><strong>Blickinteraktion:</strong> Navigieren Sie durch Menüs und ändern Sie Umgebungen (z. B. Berg zu Strand) einfach durch Ansehen von Symbolen.</li><li><strong>Gyroskopisches Gehen:</strong> Erkunden Sie 360-Grad-Umgebungen mit einer einfachen Kopfneigung um 30 Grad – keine Controller erforderlich.</li><li><strong>Virtuelle 3D-Tasten:</strong> Intuitive Steuerung für Wiedergabe und Einstellungen im VR-Raum.</li><li><strong>Einfache Benutzeroberfläche:</strong> Benutzerfreundliche Schnittstelle, die für alle Altersgruppen zugänglich ist.</li></ul><h3>Anwendungsfälle</h3><ul><li><strong>Wohlbefinden der Mitarbeiter:</strong> Reduziert Stress und Burnout durch ein zugängliches Flucht- und Entspannungsinstrument.</li><li><strong>Schülerbildung:</strong> Interaktives Lernen zur Förderung von Konzentration, Gedächtnis und körperlicher Gesundheit.</li><li><strong>Lebenslanges Lernen:</strong> Personalisierte, adaptive Yoga-Sitzungen für kontinuierliche Selbstverbesserung und Wohlbefinden.</li></ul><h3>Fazit</h3><p>Wir glauben, dass YogaVision die Yogapraxis durch Innovation revolutionieren kann und Lernende aller Hintergründe dazu inspiriert, ihre körperlichen und mentalen Ziele zu erreichen.</p>",
            "purpose": "SIH (Hardware edition)",
            "duration": "5 Monate",
            "date": ""
        }
    ]
}